{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98477e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from grading import grader\n",
    "\n",
    "load_dotenv()\n",
    "openrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "# Openrouter Client\n",
    "openrouter_client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=openrouter_api_key,\n",
    ")\n",
    "\n",
    "# Qwen/Qwen2.5-7B-Instruct Client\n",
    "qwen_client = OpenAI(\n",
    "  api_key=\"EMPTY\",\n",
    "  base_url=\"https://qwen.stephenxie.com/v1\",\n",
    ")\n",
    "\n",
    "# Define Model\n",
    "model = 'qwen/qwen3-8b'\n",
    "huggingface_model = 'Qwen/Qwen3-8B'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(huggingface_model)\n",
    "\n",
    "def count_tokens(text):\n",
    "  '''\n",
    "  Count tokens in `text` using the `Qwen3-8B` tokenizer\n",
    "  '''\n",
    "  return len(tokenizer.encode(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf2ca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility Functions ##\n",
    "\n",
    "def sanitize_json_string(json_str):\n",
    "    \"\"\"\n",
    "    Cleans up common LLM JSON formatting errors.\n",
    "    \"\"\"\n",
    "    # Replace backslashes\n",
    "    sanitized_str = json_str.replace('\\\\', '\\\\\\\\')\n",
    "\n",
    "    # Fix unquoted keys\n",
    "    sanitized_str = re.sub(r'([{,]\\s*)([a-zA-Z0-9_]+)(\\s*:)', r'\\1\"\\2\"\\3', sanitized_str)\n",
    "\n",
    "    # Replace single quotes with double quotes\n",
    "    sanitized_str = re.sub(r\"'([^']*)'\", r'\"\\1\"', sanitized_str)\n",
    "\n",
    "    # Handle improperly escaped newlines and tabs\n",
    "    sanitized_str = sanitized_str.replace('\\n', '\\\\n').replace('\\t', '\\\\t')\n",
    "    \n",
    "    return sanitized_str\n",
    "\n",
    "def parse_json_response(response):\n",
    "    '''\n",
    "    Robust JSON extraction and parsing from `response` \n",
    "    Returns type and data\n",
    "    '''\n",
    "    json_pattern = re.compile(r'\\{[\\s\\S]*\\}', re.MULTILINE)\n",
    "    match = json_pattern.search(response)\n",
    "\n",
    "    if match:\n",
    "        json_str = match.group(0)\n",
    "\n",
    "        clean_json_str = json_str.strip()\n",
    "\n",
    "        # Try to directly parse first\n",
    "        try:\n",
    "            data = json.loads(clean_json_str)\n",
    "            return data.get(\"type\"), data\n",
    "        except json.JSONDecodeError as e:\n",
    "            # If it fails, sanitize\n",
    "            print(f\"Initial JSON decode failed: {e}\")\n",
    "            print(f\"Raw JSON string (repr): {repr(clean_json_str)}\")\n",
    "\n",
    "            # Correct unquoted keys\n",
    "            fixed_keys_str = re.sub(r'([{,]\\s*)([a-zA-Z0-9_]+)(\\s*:)', r'\\1\"\\2\"\\3', clean_json_str)\n",
    "\n",
    "            # Correct single quotes to double quotes.\n",
    "            fixed_quotes_str = re.sub(r\"'([^']*)'\", r'\"\\1\"', fixed_keys_str)\n",
    "\n",
    "            # Correct backslashes and other control characters.\n",
    "            fixed_escapes_str = re.sub(r'\\\\(?![\"\\\\/bfnrtu])', r'\\\\\\\\', fixed_quotes_str)\n",
    "\n",
    "            # Clean Up newlines and tabs\n",
    "            final_sanitized_str = fixed_escapes_str.replace('\\n', '\\\\n').replace('\\t', '\\\\t')\n",
    "\n",
    "            # Try to parse the fully sanitized string.\n",
    "            try:\n",
    "                data = json.loads(final_sanitized_str)\n",
    "                return data.get(\"type\"), data\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Sanitized JSON decode failed: {e}\")\n",
    "                print(f\"Malformed JSON string: {json_str}\")\n",
    "                return None, None\n",
    "    \n",
    "    print(f\"No JSON object found in response: {response}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def isParallel(inference):\n",
    "    block_type, _ = parse_json_response(inference)\n",
    "    return block_type == \"PARALLEL\"\n",
    "\n",
    "def isSerial(inference):\n",
    "    block_type, _ = parse_json_response(inference)\n",
    "    return block_type == \"SERIAL\"\n",
    "\n",
    "def isCompleted(inference):\n",
    "    block_type, _ = parse_json_response(inference)\n",
    "    return block_type == \"COMPLETED\"\n",
    "\n",
    "def track_tokens(level, output_tokens, reasoning_tokens, prompt_tokens, completion_tokens, token_stats):\n",
    "    # Track total token usage\n",
    "    if prompt_tokens > token_stats['max_prompt_tokens']:\n",
    "        token_stats['max_prompt_tokens'] = prompt_tokens\n",
    "    if completion_tokens > token_stats['max_completion_tokens']:\n",
    "        token_stats['max_completion_tokens'] = completion_tokens\n",
    "\n",
    "    token_stats[\"total_reasoning\"] += reasoning_tokens\n",
    "    token_stats[\"total_output\"] += output_tokens\n",
    "\n",
    "    # Track level-wise\n",
    "    if level not in token_stats[\"by_level\"]:\n",
    "        token_stats[\"by_level\"][level] = {\"output\": 0, \"reasoning\": 0, \"calls\": 0}\n",
    "    \n",
    "    token_stats[\"by_level\"][level][\"output\"] += output_tokens\n",
    "    token_stats[\"by_level\"][level][\"reasoning\"] += reasoning_tokens\n",
    "    token_stats[\"by_level\"][level][\"calls\"] += 1\n",
    "\n",
    "def extract_answer(answer, client):\n",
    "    r\"\"\"\n",
    "    Use regex to extract the contents of \\\\boxed{...}\n",
    "\n",
    "    regex pattern: \\\\boxed\\{([^{}]*(?:\\{[^{}]*\\}[^{}]*)*)\\}\n",
    "\n",
    "    1. \\\\boxed\\{  \n",
    "        \\\\ - Matches a literal backslash (escaped because \\ is a special character in regex)\n",
    "        boxed - Matches the literal text \"boxed\"\n",
    "        \\{ - Matches a literal opening brace { (escaped because { is a special character in regex)\n",
    "\n",
    "    2. ([^{}]*(?:\\{[^{}]*\\}[^{}]*)*) (The main capture group)\n",
    "        2a. [^{}]*\n",
    "            [^{}] - Character class that matches any character EXCEPT { or }\n",
    "            * - Zero or more of the preceding character class\n",
    "        2b. (?:\\{[^{}]*\\}[^{}]*)*\n",
    "            (?:...) - Non-capturing group (groups the pattern but doesn't create a separate capture)\n",
    "            \\{ - Matches a literal opening brace {\n",
    "            [^{}]* - Matches zero or more characters that aren't braces\n",
    "            \\} - Matches a literal closing brace }\n",
    "            [^{}]* - Matches zero or more non-brace characters after the closing brace\n",
    "            * - The whole non-capturing group can repeat zero or more times\n",
    "\n",
    "    3. \\}\n",
    "        \\} - Matches the final literal closing brace }\n",
    "    \"\"\"\n",
    "\n",
    "    answer_pattern = r'\\\\boxed\\{([^{}]*(?:\\{[^{}]*\\}[^{}]*)*)\\}'\n",
    "    matches = re.findall(answer_pattern, answer)\n",
    "\n",
    "    if matches:\n",
    "        return matches[-1]\n",
    "    else: # Default to model extraction, if regex fails\n",
    "        extraction_prompt = ''' \n",
    "            Extract the contents of the final \\\\boxed{} and return the value, and only this value.\n",
    "        '''\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"qwen/qwen-2.5-7b-instruct\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": extraction_prompt\n",
    "                }, \n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": answer #TODO: should this be json.dumps(answer) ?\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "def get_baseline_stats(question, client=None, model=model):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\n",
    "            \"role\": \"user\", \"content\": question\n",
    "        }]\n",
    "    )\n",
    "\n",
    "    # Get the response text\n",
    "    inference = completion.choices[0].message.content\n",
    "\n",
    "    # Track token usage\n",
    "    token_usage = {\"total_output\": 0, \"total_reasoning\": 0}\n",
    "    total_completion_tokens = completion.usage.completion_tokens\n",
    "    token_usage['total_output'] = count_tokens(inference)\n",
    "    token_usage['total_reasoning'] = total_completion_tokens - token_usage['total_output']\n",
    "    \n",
    "    return inference, token_usage\n",
    "\n",
    "## Prompt Functions ##\n",
    "\n",
    "def create_math500_prompt(question, forkJoin=False):\n",
    "    '''\n",
    "    Used by the authors of the math-500 evaluation, in order to use the PRM800K Parsing logic\n",
    "    https://www.vals.ai/benchmarks/math500-03-11-2025\n",
    "    '''\n",
    "\n",
    "    forkJoinFormatting = '''\n",
    "    **IMPORTANT**\n",
    "        Note that this \\\\boxed answer term should occur ONLY inside of the COMPLETED block at recursion level 0\n",
    "    ''' if forkJoin else ''\n",
    "\n",
    "    prompt = f''' \n",
    "    Answer the following math question, given in LaTeX format, clearly and concisely, and present the final answer as:\n",
    "    \\\\(\\\\boxed{{x}}\\\\), where X is the fully simplified solution.\n",
    "\n",
    "    {forkJoinFormatting}\n",
    "\n",
    "    Example:\n",
    "        **Question:** \\\\(\\\\int_0^1 (3x^2 + 2x) \\\\, dx\\\\)\n",
    "        **Solution: \\\\(\\\\int (3x^2 + 2x) \\\\,dx = x^3 + x^2 + C\\\\) \n",
    "            Evaluating from 0 to 1: \\\\((1^3 + 1^2) - (0^3 + 0^2) = 1 + 1 - 0 = 2 \\\\boxed{2}\\\\)\n",
    "        ** Answer: \\\\boxed{2}\n",
    "\n",
    "    Now, solve the following question: \n",
    "    {question}\n",
    "    '''\n",
    "    return prompt\n",
    "\n",
    "def createSystemPrompt(current_depth, max_depth=2):\n",
    "    if current_depth == max_depth:\n",
    "        prompt = f''' \n",
    "        You are an expert model that decomposes complex tasks into parallel subtasks, \n",
    "        and uses recursive calls, structured in the following format, to evaluate and execute these subtasks.\n",
    "\n",
    "        You are at the maximum recursion depth ({current_depth}/{max_depth}) allowed for this instance, and therefore must abide by more restrictive output formatting guidelines\n",
    "\n",
    "        TASK: Analyze the provided problem and format your response in EXACTLY the provided format\n",
    "\n",
    "        **RULES**\n",
    "\n",
    "        0. **CRUCIAL**: When writing LaTeX inside a JSON string, you **MUST** escape the backslash character `\\\\`. This means you must write `\\\\\\\\` instead of `\\\\`. \n",
    "            For example, to write `\\\\sqrt{{x}}`, you must type it as `\"\\\\\\\\sqrt{{x}}\"`\n",
    "        1.  **You MUST respond with only a single, valid JSON object.** Your entire response must be the JSON object itself and nothing else.\n",
    "        2.  **ALWAYS** use double quotes for all property names and string values, i.e. always use \\\" instead of \\' for JSON formatting\n",
    "        3.  **DO NOT** under any circumstances include introductory phrases like \"Here is the JSON:\" or any other explanatory text.\n",
    "        4.  **DO NOT** under any circumstances include any text formatting (e.g. tab or newline characters) in your response; ensuring that your response is pure json\n",
    "        5.  **ENSURE** that ALL brackets are properly opened and closed\n",
    "        6.  Continue until COMPLETED at level={current_depth}\n",
    "\n",
    "        **OUTPUT FORMATTING**\n",
    "\n",
    "        {{\n",
    "            \"type\": \"SERIAL\",\n",
    "            \"level\": {current_depth},\n",
    "            \"inference\": \"relevant serial inference\"\n",
    "        }}\n",
    "        \n",
    "        {{\n",
    "            \"type\": \"COMPLETED\",\n",
    "            \"level\": {current_depth},\n",
    "            \"result\": \"complete solution to the query\"\n",
    "        }}\n",
    "        \n",
    "        Output ONLY valid JSON, no other text.\n",
    "        '''\n",
    "    else:\n",
    "        prompt = f'''\n",
    "        You are an expert model that decomposes complex tasks into parallel subtasks, \n",
    "        and uses recursive calls, structured in the following JSON format, to evaluate and execute these subtasks.\n",
    "\n",
    "        Current recursion depth: {current_depth}/{max_depth}\n",
    "\n",
    "        TASK: Analyze the provided problem and format your response in EXACTLY the provided JSON format;\n",
    "\n",
    "        **RULES**\n",
    "\n",
    "        **You MUST respond with only a single, valid JSON object.** Your entire response **MUST** be the JSON object itself and **NOTHING ELSE**.\n",
    "\n",
    "        0. **ABSOLUTELY CRITICAL**: When writing LaTeX inside a JSON string, you **MUST** escape the backslash character `\\\\`. This means you must write `\\\\\\\\` instead of `\\\\`. \n",
    "            For example, to write `\\\\sqrt{{x}}`, you must type it as `\"\\\\\\\\sqrt{{x}}\"`\n",
    "        1.  **ABSOLUTELY CRITICAL**: ALWAYS use double quotes for **ALL** property names and string values, i.e. always use \\\" instead of \\' for JSON formatting\n",
    "        2. **You MUST respond with only a single, valid JSON object.** Your entire response must be the JSON object itself and nothing else.\n",
    "        3.  **DO NOT** under any circumstances include introductory phrases like \"Here is the JSON:\" or any other explanatory text.\n",
    "        4.  **DO NOT** under any circumstances include any text formatting (e.g. tab or newline characters) in your response; ensuring that your response is pure json\n",
    "        5.  **ENSURE** that ALL brackets are properly opened and closed\n",
    "        6.  Continue until COMPLETED at level={current_depth}\n",
    "\n",
    "        **DECOMPOSITION GUIDELINES**\n",
    "\n",
    "        - Each fork must be fully self-contained and each fork's input must represent the entirety of the question, solvable without any additional context\n",
    "        - ONLY decompose tasks when subtasks are independent AND require substantial work (>30 seconds of human effort)\n",
    "            - PARALLEL tasks may be used to investigate possible avenues for solutions, especially if there is not one clear path forward\n",
    "        - DO NOT decompose basic mathematical operations, nor single-step algebraic manipulations (substitution, solving for one variable, simple derivatives, etc.)\n",
    "        - SERIAL blocks may contain either \n",
    "            1. an integration of the previous PARALLEL block(s), where work is still required in solving the problem\n",
    "            2. a step in the solution of the problem that is only possible to execute serially, as it depends on prior steps or informs future steps\n",
    "\n",
    "        **OUTPUT FORMATTING**\n",
    "\n",
    "        {{\n",
    "            \"type\": \"PARALLEL\",\n",
    "            \"level\": {current_depth},\n",
    "            \"forks\": [\n",
    "                {{\"input\": \"specific self-contained question\"}},\n",
    "                {{\"input\": \"another independent question\"}}\n",
    "            ]\n",
    "        }}\n",
    "\n",
    "        {{\n",
    "            \"type\": \"SERIAL\",\n",
    "            \"level\": {current_depth},\n",
    "            \"inference\": \"relevant serial inference\"\n",
    "        }}\n",
    "\n",
    "        {{\n",
    "            \"type\": \"COMPLETED\",\n",
    "            \"level\": {current_depth},\n",
    "            \"result\": \"The answer is \\\\\\\\boxed{{x}}\"\n",
    "        }}\n",
    "\n",
    "        **DO NOT** deviate from this format. All keys and string values must be enclosed in double quotes. All backslashes must be escaped like this: \\\\\\\\\"\n",
    "        '''\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def createStatePrompt(question, partial_answer=None):\n",
    "    prompt = f'''\n",
    "    Task:\n",
    "    \n",
    "    {question}\n",
    "\n",
    "    ####\n",
    "    \n",
    "    Execute the next logical step {{one of: PARALLEL, SERIAL, COMPLETED}} in solving this problem.\n",
    "    \n",
    "    **CRUCIAL**\n",
    "        Produce exactly one valid JSON object that represents the next step forward.\n",
    "\n",
    "    The response progress up until this point is shown below:\n",
    "\n",
    "    {partial_answer if partial_answer is not None else \"\"}\n",
    "    '''\n",
    "\n",
    "    return prompt\n",
    "\n",
    "## Parallel/Recursive LLM Functions ##\n",
    "\n",
    "def processParallelBlock(inference, current_depth, max_depth, client, token_stats):\n",
    "    '''\n",
    "    Extract and process the PARALLEL block in the inference\n",
    "    '''\n",
    "    block_type, data = parse_json_response(inference)\n",
    "    \n",
    "    if block_type != \"PARALLEL\":\n",
    "        return \"\", \"\"\n",
    "\n",
    "    forks = data.get(\"forks\", [])\n",
    "\n",
    "    processed_forks, complete_processed_forks = [], []\n",
    "    \n",
    "    for fork in forks:\n",
    "        question = fork.get(\"input\", \"\")\n",
    "        if not question:\n",
    "            continue\n",
    "            \n",
    "        # Recursive processing call\n",
    "        completed_block, complete_trace, _ = llmForkJoin(question, current_depth + 1, max_depth, client, token_stats=token_stats)\n",
    "\n",
    "        final_result = None\n",
    "        try:\n",
    "            parsed_block = json.loads(completed_block.strip())\n",
    "            final_result = parsed_block.get('result')\n",
    "        except Exception as e:\n",
    "            final_result = {\n",
    "                \"type\": \"ERROR\",\n",
    "                \"level\": current_depth,\n",
    "                \"message\": f\"An error occurred: {str(e)}\",\n",
    "                \"raw\": completed_block\n",
    "            }\n",
    "\n",
    "        # Parse the JSON strings back to objects\n",
    "        complete_trace_obj = json.loads(complete_trace) if complete_trace else {}\n",
    "\n",
    "        processed_forks.append({'question': question, 'answer': final_result})\n",
    "        complete_processed_forks.append({'question': question, 'answer': complete_trace_obj})\n",
    "\n",
    "    # Construct processed PARALLEL block\n",
    "    processed_parallel_block = {\n",
    "        \"type\": \"PARALLEL\",\n",
    "        \"level\": current_depth,\n",
    "        \"forks\": [{\"input\": fork[\"question\"], \"output\": fork[\"answer\"]} for fork in processed_forks]\n",
    "    }\n",
    "\n",
    "    # Construct complete processed PARALLEL block\n",
    "    complete_processed_parallel_block = {\n",
    "        \"type\": \"PARALLEL\", \n",
    "        \"level\": current_depth,\n",
    "        \"forks\": [{\"input\": fork[\"question\"], \"output\": fork[\"answer\"]} for fork in complete_processed_forks]\n",
    "    }\n",
    "\n",
    "    return json.dumps(processed_parallel_block), json.dumps(complete_processed_parallel_block)\n",
    "\n",
    "def llmForkJoin(question, current_depth=0, max_depth=2, client=None, model=model, token_stats=None):\n",
    "    '''\n",
    "    Main function to handle recursive parallel task decomposition\n",
    "    '''\n",
    "\n",
    "    if token_stats is None:\n",
    "        token_stats = {\n",
    "            \"total_output\": 0, \n",
    "            \"total_reasoning\": 0, \n",
    "            \"max_prompt_tokens\": 0, \n",
    "            \"max_completion_tokens\": 0, \n",
    "            \"by_level\": {}\n",
    "        }\n",
    "\n",
    "    if current_depth > max_depth:\n",
    "        obj = {\n",
    "            \"type\": \"COMPLETED\", \n",
    "            \"level\": current_depth, \n",
    "            \"result\": \"Maximum Recursion Depth Exceeded! Stopping Inference.\"\n",
    "        }\n",
    "        return json.dumps(obj), \"\", token_stats\n",
    "\n",
    "    # Create 'system' and 'user' prompts\n",
    "    system_prompt = createSystemPrompt(current_depth, max_depth)\n",
    "    state_prompt = createStatePrompt(question)\n",
    "    accumulated_response, complete_accumulated_response, completed_block = [], [], \"\"\n",
    "\n",
    "    # Initial context for the conversation\n",
    "    current_context = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": state_prompt}\n",
    "    ]\n",
    "\n",
    "    while True:        \n",
    "        try:\n",
    "            ## Model Interactions ##\n",
    "            # Create the model chat.completions object\n",
    "            completion = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=current_context\n",
    "            )\n",
    "\n",
    "            # Get the response text\n",
    "            new_inference = completion.choices[0].message.content.strip()\n",
    "\n",
    "            ## Track Token Usage ##\n",
    "            # Get and cache the total number of output_tokens used\n",
    "            total_output_tokens = completion.usage.completion_tokens\n",
    "            output_tokens = count_tokens(new_inference)\n",
    "            reasoning_tokens = total_output_tokens - output_tokens\n",
    "            prompt_tokens = completion.usage.prompt_tokens\n",
    "\n",
    "            track_tokens(current_depth, output_tokens, reasoning_tokens, prompt_tokens, total_output_tokens, token_stats)\n",
    "\n",
    "            ## Handle Model Response ##\n",
    "            # Check if the inference is completed\n",
    "            if isCompleted(new_inference):\n",
    "                complete_accumulated_response.append(json.loads(new_inference))\n",
    "                completed_block = new_inference\n",
    "                break\n",
    "            # Check if the new inference contains a PARALLEL block\n",
    "            elif isParallel(new_inference):\n",
    "                processed_block, complete_processed_block = processParallelBlock(new_inference, current_depth, max_depth, client, token_stats)\n",
    "                accumulated_response.append(json.loads(processed_block))\n",
    "                complete_accumulated_response.append(json.loads(complete_processed_block))\n",
    "            # Check if the new inference contains a SERIAL block\n",
    "            elif isSerial(new_inference):\n",
    "                accumulated_response.append(json.loads(new_inference))\n",
    "                complete_accumulated_response.append(json.loads(new_inference))\n",
    "            # Otherwise break due to invalid JSON tag\n",
    "            else:\n",
    "                error_obj = {\n",
    "                    \"type\": \"ERROR\", \n",
    "                    \"level\": current_depth, \n",
    "                    \"message\": \"INVALID JSON FORMATTING\",\n",
    "                    \"raw\": new_inference\n",
    "                }\n",
    "                accumulated_response.append(error_obj)\n",
    "                complete_accumulated_response.append(error_obj)\n",
    "\n",
    "                # Print statement for error monitoring\n",
    "                print(\"ERROR: Invalid JSON was produced by the model, exiting current inference.\")\n",
    "                break\n",
    "\n",
    "            # Update conversation context for continuation\n",
    "            current_context = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": createStatePrompt(question, json.dumps(accumulated_response, indent=2))}\n",
    "            ]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in llmForkJoin at depth {current_depth}: {e}\")\n",
    "            error_obj = {\n",
    "                \"type\": \"ERROR\",\n",
    "                \"level\": current_depth,\n",
    "                \"message\": f\"An error occurred: {str(e)}\"\n",
    "            }\n",
    "            complete_accumulated_response.append(error_obj)\n",
    "            completed_block = json.dumps(error_obj)\n",
    "\n",
    "    return completed_block, json.dumps(complete_accumulated_response, indent=2), token_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2837c70b",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3075a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'If $f(x) = \\\\frac{3x-2}{x-2}$, what is the value of $f(-2) +f(-1)+f(0)$? Express your answer as a common fraction.'\n",
    "answer = '\\\\frac{14}{3}'\n",
    "\n",
    "question = create_math500_prompt(question, True)\n",
    "completed_block, accumulated_inference, token_usage = llmForkJoin(question, client=openrouter_client, model=model)\n",
    "\n",
    "print('## Decomposition Inference ##\\n', accumulated_inference, '\\n')\n",
    "print('## Decomposition Tokens ##\\n', token_usage, '\\n')\n",
    "\n",
    "decomp_soln = extract_answer(completed_block, client=openrouter_client)\n",
    "\n",
    "print('Decomposition Correct?: ', grader.grade_answer(decomp_soln, answer))\n",
    "\n",
    "inf, baseline_token_usage = get_baseline_stats(create_math500_prompt(question, False), client=openrouter_client, model=model)\n",
    "print('## Baseline Inference ##\\n', inf, '\\n')\n",
    "print('## Baseline Tokens##\\n', baseline_token_usage, '\\n')\n",
    "\n",
    "baseline_soln = extract_answer(inf, openrouter_client)\n",
    "\n",
    "print('Baseline Correct?: ', grader.grade_answer(baseline_soln, answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cc985c",
   "metadata": {},
   "source": [
    "#### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5173650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Dataset\n",
    "ds = load_dataset(\"math-ai/math500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7969189",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list, count = [], 1\n",
    "\n",
    "for row in ds['test']:\n",
    "    print(f\"Starting problem {row['unique_id']}, {count}/500\")\n",
    "    \n",
    "    resp_dict = {'problem': row['problem'], 'answer': row['answer']}\n",
    "    problem = row['problem']\n",
    "\n",
    "    print(\"Correct Solution: \", row['answer'])\n",
    "\n",
    "    ## forkJoin Inference/Analysis ##\n",
    "    forkJoin_question = create_math500_prompt(problem, True)\n",
    "    completed_block, accumulated_inference, token_usage = llmForkJoin(forkJoin_question, client=openrouter_client, model=model)\n",
    "    resp_dict['decomp_inference'] = accumulated_inference\n",
    "    resp_dict['decomp_tokens'] = token_usage\n",
    "\n",
    "    # Print Decomposition Inference for Monitoring\n",
    "    print('## Decomposition Inference ##\\n', accumulated_inference)\n",
    "\n",
    "    extracted_answer = extract_answer(completed_block, openrouter_client)\n",
    "    resp_dict['decomp_extracted_answer'] = extracted_answer\n",
    "    resp_dict['decomp_correct'] = grader.grade_answer(extracted_answer, row['answer'])\n",
    "\n",
    "    print(f\"\\tCompleted Decomposition Inference and Analysis - Tokens: {token_usage['total_output'] + token_usage['total_reasoning']} - Correct: {resp_dict['decomp_correct']}\\n\")\n",
    "\n",
    "    # ## Baseline Inference/Analysis ##\n",
    "    # baseline_question = create_math500_prompt(problem, False)\n",
    "    # baseline_inference, baseline_token_usage = get_baseline_stats(baseline_question, client=openrouter_client, model=model)\n",
    "    # resp_dict['baseline_inference'] = baseline_inference\n",
    "    # resp_dict['baseline_tokens'] = baseline_token_usage\n",
    "\n",
    "    # baseline_extracted_answer = extract_answer(baseline_inference, openrouter_client)\n",
    "    # resp_dict['baseline_extracted_answer'] = baseline_extracted_answer\n",
    "    # resp_dict['baseline_correct'] = grader.grade_answer(baseline_extracted_answer, row['answer'])\n",
    "\n",
    "    # print(f\"\\tCompleted Baseline Inference and Analysis - Tokens: {baseline_token_usage['total_output'] + baseline_token_usage['total_reasoning']} - Correct: {resp_dict['baseline_correct']}\")\n",
    "    \n",
    "    # Append to df_list\n",
    "    df_list.append(resp_dict)\n",
    "\n",
    "    # Increment tally\n",
    "    count += 1\n",
    "\n",
    "    # Save every 25 rows\n",
    "    if count % 25 == 0:\n",
    "        partial_df = pd.DataFrame(df_list)\n",
    "        partial_df.to_csv('results.csv')\n",
    "\n",
    "        # Clear jupyter Output\n",
    "        clear_output(wait=False)\n",
    "\n",
    "df = pd.DataFrame(df_list)\n",
    "df.to_csv('results.csv')\n",
    "\n",
    "## Note: Saved until 299/500 - restart from 300/500 [test/geometry/1097.json]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd47249",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4585266",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"results_0_299.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b171ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the number of correct answers\n",
    "print(\"Correct: \", df[df[\"decomp_correct\"] == True].shape[0] / df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a7aed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomp_df = df[df['decomp_inference'].str.contains(\"PARALLEL\")]\n",
    "print(\"Decomposition Percentage: \", decomp_df.shape[0] / df.shape[0])\n",
    "print(\"Decomposition Accuracy: \", decomp_df[decomp_df[\"decomp_correct\"] == True].shape[0] / decomp_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2dbcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decomp_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86feb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomp_df.to_csv(\"recursive_llm_decompositions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58707b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_df = df[df['decomp_inference'].str.contains(\"ERROR\")]\n",
    "print(\"ERROR Percentage: \", err_df.shape[0] / df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15bc014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
